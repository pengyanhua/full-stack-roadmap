# æ•°æ®ç®¡é“æ¶æ„è®¾è®¡

## ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [Airflowå®Œæ•´ç¤ºä¾‹](#airflowå®Œæ•´ç¤ºä¾‹)
- [CDCå®æ—¶æ•°æ®æ•è·](#cdcå®æ—¶æ•°æ®æ•è·)
- [Flinkæµå¤„ç†](#flinkæµå¤„ç†)
- [æ•°æ®è´¨é‡æ£€æŸ¥](#æ•°æ®è´¨é‡æ£€æŸ¥)
- [ç›‘æ§ä¸å‘Šè­¦](#ç›‘æ§ä¸å‘Šè­¦)
- [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

## æ¦‚è¿°

### æ•°æ®ç®¡é“æ¶æ„æ¼”è¿›

```
ä¼ ç»Ÿæ‰¹å¤„ç†          æ··åˆæ¶æ„           å®æ—¶æµå¤„ç†
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å®šæ—¶ä»»åŠ¡  â”‚       â”‚  Airflow  â”‚      â”‚  Flink   â”‚
â”‚  Cron    â”‚  -->  â”‚  +       â”‚ -->  â”‚  Kafka   â”‚
â”‚  ETL     â”‚       â”‚  CDC     â”‚      â”‚  å®æ—¶å¤„ç† â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  T+1å»¶è¿Ÿ           å°æ—¶çº§å»¶è¿Ÿ         ç§’çº§å»¶è¿Ÿ
```

### ç°ä»£æ•°æ®ç®¡é“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ•°æ®æºå±‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MySQL   â”‚PostgreSQLâ”‚ MongoDB  â”‚   API    â”‚   æ—¥å¿—æ–‡ä»¶        â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚          â”‚          â”‚          â”‚               â”‚
     v          v          v          v               v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®é‡‡é›†å±‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Debeziumâ”‚ Maxwell  â”‚ Sqoop    â”‚ Flume    â”‚  Filebeat        â”‚
â”‚  CDC    â”‚   CDC    â”‚  æ‰¹é‡     â”‚  æ—¥å¿—     â”‚   æ—¥å¿—           â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚         â”‚          â”‚          â”‚          â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ¶ˆæ¯é˜Ÿåˆ—å±‚                                 â”‚
â”‚              Kafka (3 Brokers, RF=3)                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚  Topic1  â”‚  Topic2  â”‚  Topic3  â”‚  Topic4  â”‚            â”‚
â”‚   â”‚ order    â”‚  user    â”‚  product â”‚   log    â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚          â”‚          â”‚          â”‚
         v          v          v          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµå¤„ç†å±‚                                   â”‚
â”‚         Flink Cluster (JobManager + TaskManager)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ æ¸…æ´—è½¬æ¢  â”‚  èšåˆè®¡ç®— â”‚ å…³è”Join  â”‚  çª—å£åˆ†æ         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚          â”‚          â”‚          â”‚
        v          v          v          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å­˜å‚¨å±‚                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hive    â”‚ ClickHouseâ”‚  Doris  â”‚  Redis   â”‚  Elasticsearch   â”‚
â”‚ æ•°æ®æ¹–   â”‚   OLAP   â”‚  å®æ—¶ä»“  â”‚  ç¼“å­˜     â”‚   æœç´¢å¼•æ“        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Airflowå®Œæ•´ç¤ºä¾‹

### Airflowæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Airflow Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Web    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤ Metadata â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Schedulerâ”‚    â”‚
â”‚  â”‚  Server  â”‚         â”‚ Database â”‚        â”‚          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚(Postgres)â”‚        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â–²               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚           â”‚
â”‚       â”‚                                        â”‚           â”‚
â”‚       â”‚                                        v           â”‚
â”‚       â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      Task Queue          â”‚      â”‚
â”‚                       â”‚      (Redis/RabbitMQ)      â”‚      â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                    â”‚                       â”‚
â”‚                                    v                       â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚               â”‚         Executor Pool              â”‚       â”‚
â”‚               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚               â”‚Worker 1â”‚Worker 2â”‚Worker 3â”‚Worker Nâ”‚       â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®Œæ•´DAGç¤ºä¾‹ï¼šç”µå•†è®¢å•ETL

```python
"""
ç”µå•†è®¢å•æ•°æ®ETLç®¡é“
åŠŸèƒ½ï¼š
1. ä»MySQLæå–è®¢å•æ•°æ®
2. æ•°æ®æ¸…æ´—ä¸è½¬æ¢
3. å…³è”ç”¨æˆ·å’Œäº§å“ä¿¡æ¯
4. å†™å…¥æ•°æ®ä»“åº“
5. æ•°æ®è´¨é‡æ£€æŸ¥
6. å‘Šè­¦é€šçŸ¥
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.mysql.operators.mysql import MySqlOperator
from airflow.providers.amazon.aws.transfers.mysql_to_s3 import MySQLToS3Operator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta
import logging
import pandas as pd
import great_expectations as ge

# é»˜è®¤å‚æ•°é…ç½®
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email': ['data-alerts@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=2),
    'sla': timedelta(hours=3),
}

# åˆ›å»ºDAG
dag = DAG(
    'ecommerce_order_etl',
    default_args=default_args,
    description='ç”µå•†è®¢å•æ•°æ®ETLç®¡é“',
    schedule_interval='0 2 * * *',  # æ¯å¤©å‡Œæ™¨2ç‚¹è¿è¡Œ
    catchup=False,
    max_active_runs=1,
    tags=['etl', 'orders', 'production'],
)

# ============================================================================
# ä»»åŠ¡1ï¼šæ£€æŸ¥ä¸Šæ¸¸ä¾èµ–
# ============================================================================
check_upstream = ExternalTaskSensor(
    task_id='check_upstream_dag',
    external_dag_id='user_data_etl',
    external_task_id='load_to_warehouse',
    allowed_states=['success'],
    failed_states=['failed', 'skipped'],
    mode='reschedule',
    timeout=3600,
    poke_interval=300,
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡2ï¼šæ•°æ®æå–
# ============================================================================
def extract_orders(**context):
    """ä»MySQLæå–è®¢å•æ•°æ®"""
    from airflow.providers.mysql.hooks.mysql import MySqlHook

    execution_date = context['execution_date']
    prev_date = execution_date - timedelta(days=1)

    mysql_hook = MySqlHook(mysql_conn_id='mysql_orders_db')

    # å¢é‡æå–SQL
    sql = f"""
    SELECT
        order_id,
        user_id,
        product_id,
        quantity,
        price,
        total_amount,
        order_status,
        payment_method,
        shipping_address,
        created_at,
        updated_at
    FROM orders
    WHERE DATE(created_at) = '{prev_date.strftime('%Y-%m-%d')}'
        AND is_deleted = 0
    """

    # æ‰§è¡ŒæŸ¥è¯¢
    df = mysql_hook.get_pandas_df(sql)

    # ä¿å­˜åˆ°ä¸´æ—¶ä½ç½®
    output_path = f'/tmp/orders_{prev_date.strftime("%Y%m%d")}.parquet'
    df.to_parquet(output_path, compression='snappy', index=False)

    logging.info(f"æå–äº† {len(df)} æ¡è®¢å•è®°å½•")

    # æ¨é€å…ƒæ•°æ®åˆ°XCom
    context['task_instance'].xcom_push(
        key='order_count',
        value=len(df)
    )
    context['task_instance'].xcom_push(
        key='output_path',
        value=output_path
    )

    return output_path

extract_orders_task = PythonOperator(
    task_id='extract_orders',
    python_callable=extract_orders,
    provide_context=True,
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡3ï¼šæ•°æ®æ¸…æ´—ä¸è½¬æ¢
# ============================================================================
def clean_and_transform(**context):
    """æ•°æ®æ¸…æ´—ä¸è½¬æ¢"""
    import numpy as np

    # ä»XComè·å–ä¸Šæ¸¸æ•°æ®
    ti = context['task_instance']
    input_path = ti.xcom_pull(task_ids='extract_orders', key='output_path')

    # è¯»å–æ•°æ®
    df = pd.read_parquet(input_path)

    logging.info(f"åŸå§‹æ•°æ®: {len(df)} è¡Œ")

    # 1. åˆ é™¤é‡å¤æ•°æ®
    df = df.drop_duplicates(subset=['order_id'], keep='last')

    # 2. å¤„ç†ç¼ºå¤±å€¼
    df['shipping_address'] = df['shipping_address'].fillna('Unknown')
    df['payment_method'] = df['payment_method'].fillna('Unknown')

    # 3. æ•°æ®ç±»å‹è½¬æ¢
    df['order_id'] = df['order_id'].astype('int64')
    df['user_id'] = df['user_id'].astype('int64')
    df['product_id'] = df['product_id'].astype('int64')
    df['quantity'] = df['quantity'].astype('int32')
    df['price'] = df['price'].astype('float64')
    df['total_amount'] = df['total_amount'].astype('float64')

    # 4. æ•°æ®éªŒè¯
    # éªŒè¯é‡‘é¢è®¡ç®—
    df['calculated_amount'] = df['quantity'] * df['price']
    df['amount_diff'] = np.abs(df['total_amount'] - df['calculated_amount'])

    # æ ‡è®°å¼‚å¸¸æ•°æ®
    df['is_amount_valid'] = df['amount_diff'] < 0.01
    invalid_count = (~df['is_amount_valid']).sum()

    if invalid_count > 0:
        logging.warning(f"å‘ç° {invalid_count} æ¡é‡‘é¢å¼‚å¸¸æ•°æ®")

    # 5. æ·»åŠ æ´¾ç”Ÿå­—æ®µ
    df['order_date'] = pd.to_datetime(df['created_at']).dt.date
    df['order_hour'] = pd.to_datetime(df['created_at']).dt.hour
    df['order_day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek

    # 6. æ•°æ®åˆ†ç±»
    df['price_category'] = pd.cut(
        df['price'],
        bins=[0, 50, 100, 500, float('inf')],
        labels=['low', 'medium', 'high', 'premium']
    )

    # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    output_path = input_path.replace('.parquet', '_cleaned.parquet')
    df.to_parquet(output_path, compression='snappy', index=False)

    logging.info(f"æ¸…æ´—åæ•°æ®: {len(df)} è¡Œ")

    # æ¨é€ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_orders': len(df),
        'invalid_amount_count': int(invalid_count),
        'total_revenue': float(df['total_amount'].sum()),
        'avg_order_value': float(df['total_amount'].mean()),
    }

    ti.xcom_push(key='cleaned_path', value=output_path)
    ti.xcom_push(key='stats', value=stats)

    return output_path

clean_transform_task = PythonOperator(
    task_id='clean_and_transform',
    python_callable=clean_and_transform,
    provide_context=True,
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡4ï¼šå…³è”ç»´åº¦æ•°æ®
# ============================================================================
def join_dimensions(**context):
    """å…³è”ç”¨æˆ·å’Œäº§å“ç»´åº¦ä¿¡æ¯"""
    from airflow.providers.postgres.hooks.postgres import PostgresHook

    ti = context['task_instance']
    input_path = ti.xcom_pull(task_ids='clean_and_transform', key='cleaned_path')

    # è¯»å–è®¢å•æ•°æ®
    orders_df = pd.read_parquet(input_path)

    # è·å–ç»´åº¦æ•°æ®
    pg_hook = PostgresHook(postgres_conn_id='dwh_postgres')

    # è·å–ç”¨æˆ·ç»´åº¦
    users_df = pg_hook.get_pandas_df("""
        SELECT user_id, user_name, user_level, register_date, city
        FROM dim_users
        WHERE is_active = true
    """)

    # è·å–äº§å“ç»´åº¦
    products_df = pg_hook.get_pandas_df("""
        SELECT product_id, product_name, category, brand, cost_price
        FROM dim_products
        WHERE is_active = true
    """)

    # å…³è”æ•°æ®
    result_df = orders_df.merge(
        users_df,
        on='user_id',
        how='left'
    ).merge(
        products_df,
        on='product_id',
        how='left'
    )

    # è®¡ç®—åˆ©æ¶¦
    result_df['profit'] = (result_df['price'] - result_df['cost_price']) * result_df['quantity']
    result_df['profit_margin'] = result_df['profit'] / result_df['total_amount'] * 100

    # ä¿å­˜ç»“æœ
    output_path = input_path.replace('_cleaned.parquet', '_enriched.parquet')
    result_df.to_parquet(output_path, compression='snappy', index=False)

    logging.info(f"å…³è”åæ•°æ®: {len(result_df)} è¡Œ, {result_df.shape[1]} åˆ—")

    ti.xcom_push(key='enriched_path', value=output_path)

    return output_path

join_dimensions_task = PythonOperator(
    task_id='join_dimensions',
    python_callable=join_dimensions,
    provide_context=True,
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡5ï¼šæ•°æ®è´¨é‡æ£€æŸ¥
# ============================================================================
def validate_data_quality(**context):
    """ä½¿ç”¨Great Expectationsè¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥"""
    ti = context['task_instance']
    input_path = ti.xcom_pull(task_ids='join_dimensions', key='enriched_path')

    # è¯»å–æ•°æ®
    df = pd.read_parquet(input_path)

    # è½¬æ¢ä¸ºGE DataFrame
    ge_df = ge.from_pandas(df)

    # å®šä¹‰æ•°æ®è´¨é‡è§„åˆ™
    validation_results = []

    # 1. æ£€æŸ¥ä¸»é”®å”¯ä¸€æ€§
    result1 = ge_df.expect_column_values_to_be_unique('order_id')
    validation_results.append(('å”¯ä¸€æ€§æ£€æŸ¥', result1['success']))

    # 2. æ£€æŸ¥å¿…å¡«å­—æ®µ
    for col in ['user_id', 'product_id', 'quantity', 'price']:
        result = ge_df.expect_column_values_to_not_be_null(col)
        validation_results.append((f'{col}éç©ºæ£€æŸ¥', result['success']))

    # 3. æ£€æŸ¥æ•°å€¼èŒƒå›´
    result3 = ge_df.expect_column_values_to_be_between('quantity', min_value=1, max_value=1000)
    validation_results.append(('æ•°é‡èŒƒå›´æ£€æŸ¥', result3['success']))

    result4 = ge_df.expect_column_values_to_be_between('price', min_value=0, max_value=1000000)
    validation_results.append(('ä»·æ ¼èŒƒå›´æ£€æŸ¥', result4['success']))

    # 4. æ£€æŸ¥æšä¸¾å€¼
    valid_statuses = ['pending', 'paid', 'shipped', 'delivered', 'cancelled']
    result5 = ge_df.expect_column_values_to_be_in_set('order_status', valid_statuses)
    validation_results.append(('è®¢å•çŠ¶æ€æ£€æŸ¥', result5['success']))

    # 5. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    result6 = ge_df.expect_column_values_to_not_be_null('user_name')
    validation_results.append(('ç”¨æˆ·ä¿¡æ¯å®Œæ•´æ€§', result6['success']))

    result7 = ge_df.expect_column_values_to_not_be_null('product_name')
    validation_results.append(('äº§å“ä¿¡æ¯å®Œæ•´æ€§', result7['success']))

    # ç»Ÿè®¡ç»“æœ
    total_checks = len(validation_results)
    passed_checks = sum(1 for _, success in validation_results if success)

    logging.info(f"æ•°æ®è´¨é‡æ£€æŸ¥: {passed_checks}/{total_checks} é€šè¿‡")

    for check_name, success in validation_results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        logging.info(f"  {check_name}: {status}")

    # å¦‚æœå…³é”®æ£€æŸ¥å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    critical_checks = validation_results[:5]  # å‰5ä¸ªæ˜¯å…³é”®æ£€æŸ¥
    if not all(success for _, success in critical_checks):
        raise ValueError("å…³é”®æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥")

    ti.xcom_push(key='quality_check_passed', value=passed_checks)
    ti.xcom_push(key='quality_check_total', value=total_checks)

    return passed_checks

validate_quality_task = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    provide_context=True,
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡6ï¼šåŠ è½½åˆ°æ•°æ®ä»“åº“
# ============================================================================
def load_to_warehouse(**context):
    """åŠ è½½æ•°æ®åˆ°æ•°æ®ä»“åº“"""
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    import psycopg2.extras as extras

    ti = context['task_instance']
    input_path = ti.xcom_pull(task_ids='join_dimensions', key='enriched_path')
    execution_date = context['execution_date']

    # è¯»å–æ•°æ®
    df = pd.read_parquet(input_path)

    # æ·»åŠ ETLå…ƒæ•°æ®
    df['etl_batch_id'] = execution_date.strftime('%Y%m%d%H%M%S')
    df['etl_load_time'] = datetime.now()

    # è·å–æ•°æ®åº“è¿æ¥
    pg_hook = PostgresHook(postgres_conn_id='dwh_postgres')
    conn = pg_hook.get_conn()
    cursor = conn.cursor()

    try:
        # 1. åˆ›å»ºä¸´æ—¶è¡¨
        cursor.execute("""
            CREATE TEMP TABLE temp_orders (LIKE fact_orders INCLUDING ALL)
        """)

        # 2. æ‰¹é‡æ’å…¥ä¸´æ—¶è¡¨
        tuples = [tuple(x) for x in df.to_numpy()]
        cols = ','.join(list(df.columns))

        query = f"INSERT INTO temp_orders({cols}) VALUES %s"
        extras.execute_values(cursor, query, tuples)

        logging.info(f"å·²æ’å…¥ {len(df)} æ¡è®°å½•åˆ°ä¸´æ—¶è¡¨")

        # 3. Mergeåˆ°ç›®æ ‡è¡¨ (UPDATE + INSERT)
        cursor.execute("""
            -- æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
            UPDATE fact_orders fo
            SET
                order_status = t.order_status,
                updated_at = t.updated_at,
                etl_batch_id = t.etl_batch_id,
                etl_load_time = t.etl_load_time
            FROM temp_orders t
            WHERE fo.order_id = t.order_id;

            -- æ’å…¥æ–°è®°å½•
            INSERT INTO fact_orders
            SELECT * FROM temp_orders
            WHERE order_id NOT IN (SELECT order_id FROM fact_orders);
        """)

        # 4. è®°å½•åŠ è½½ç»Ÿè®¡
        cursor.execute("""
            INSERT INTO etl_load_log (
                dag_id, task_id, execution_date,
                table_name, rows_inserted, rows_updated, load_time
            ) VALUES (
                %s, %s, %s, %s,
                (SELECT COUNT(*) FROM temp_orders WHERE order_id NOT IN (SELECT order_id FROM fact_orders)),
                (SELECT COUNT(*) FROM temp_orders WHERE order_id IN (SELECT order_id FROM fact_orders)),
                %s
            )
        """, (
            dag.dag_id,
            'load_to_warehouse',
            execution_date,
            'fact_orders',
            datetime.now()
        ))

        conn.commit()
        logging.info("æ•°æ®æˆåŠŸåŠ è½½åˆ°æ•°æ®ä»“åº“")

    except Exception as e:
        conn.rollback()
        logging.error(f"åŠ è½½å¤±è´¥: {str(e)}")
        raise
    finally:
        cursor.close()
        conn.close()

    return len(df)

load_warehouse_task = PythonOperator(
    task_id='load_to_warehouse',
    python_callable=load_to_warehouse,
    provide_context=True,
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡7ï¼šæ›´æ–°ç»Ÿè®¡è¡¨
# ============================================================================
update_stats = MySqlOperator(
    task_id='update_statistics',
    mysql_conn_id='dwh_postgres',
    sql="""
        -- æ›´æ–°è®¢å•ç»Ÿè®¡è¡¨
        INSERT INTO order_daily_stats (
            stat_date,
            total_orders,
            total_revenue,
            avg_order_value,
            updated_at
        )
        SELECT
            DATE(created_at) as stat_date,
            COUNT(*) as total_orders,
            SUM(total_amount) as total_revenue,
            AVG(total_amount) as avg_order_value,
            NOW() as updated_at
        FROM fact_orders
        WHERE DATE(created_at) = '{{ ds }}'
        GROUP BY DATE(created_at)
        ON CONFLICT (stat_date) DO UPDATE SET
            total_orders = EXCLUDED.total_orders,
            total_revenue = EXCLUDED.total_revenue,
            avg_order_value = EXCLUDED.avg_order_value,
            updated_at = EXCLUDED.updated_at;
    """,
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡8ï¼šæˆåŠŸé€šçŸ¥
# ============================================================================
def send_success_notification(**context):
    """å‘é€æˆåŠŸé€šçŸ¥"""
    ti = context['task_instance']
    stats = ti.xcom_pull(task_ids='clean_and_transform', key='stats')
    quality_passed = ti.xcom_pull(task_ids='validate_data_quality', key='quality_check_passed')
    quality_total = ti.xcom_pull(task_ids='validate_data_quality', key='quality_check_total')

    message = f"""
    âœ… è®¢å•ETLç®¡é“æ‰§è¡ŒæˆåŠŸ

    æ‰§è¡Œæ—¥æœŸ: {context['ds']}
    æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

    ğŸ“Š æ•°æ®ç»Ÿè®¡:
    - è®¢å•æ•°é‡: {stats['total_orders']:,}
    - æ€»æ”¶å…¥: Â¥{stats['total_revenue']:,.2f}
    - å¹³å‡è®¢å•é‡‘é¢: Â¥{stats['avg_order_value']:,.2f}
    - å¼‚å¸¸æ•°æ®: {stats['invalid_amount_count']}

    âœ”ï¸ è´¨é‡æ£€æŸ¥: {quality_passed}/{quality_total} é€šè¿‡

    ğŸ”— DAGé“¾æ¥: {context['task_instance'].log_url}
    """

    logging.info(message)
    # è¿™é‡Œå¯ä»¥é›†æˆSlack/é’‰é’‰/ä¼ä¸šå¾®ä¿¡é€šçŸ¥

    return message

success_notification = PythonOperator(
    task_id='send_success_notification',
    python_callable=send_success_notification,
    provide_context=True,
    trigger_rule='all_success',
    dag=dag,
)

# ============================================================================
# ä»»åŠ¡9ï¼šå¤±è´¥å‘Šè­¦
# ============================================================================
def send_failure_alert(**context):
    """å‘é€å¤±è´¥å‘Šè­¦"""
    ti = context['task_instance']
    exception = context.get('exception')

    message = f"""
    âŒ è®¢å•ETLç®¡é“æ‰§è¡Œå¤±è´¥

    DAG ID: {context['dag'].dag_id}
    Task ID: {ti.task_id}
    æ‰§è¡Œæ—¥æœŸ: {context['ds']}
    æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

    é”™è¯¯ä¿¡æ¯:
    {str(exception)}

    ğŸ”— æ—¥å¿—é“¾æ¥: {ti.log_url}

    è¯·åŠæ—¶å¤„ç†ï¼
    """

    logging.error(message)
    # è¿™é‡Œå¯ä»¥é›†æˆå‘Šè­¦ç³»ç»Ÿ

    return message

failure_alert = PythonOperator(
    task_id='send_failure_alert',
    python_callable=send_failure_alert,
    provide_context=True,
    trigger_rule='one_failed',
    dag=dag,
)

# ============================================================================
# å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
# ============================================================================
check_upstream >> extract_orders_task >> clean_transform_task
clean_transform_task >> join_dimensions_task >> validate_quality_task
validate_quality_task >> load_warehouse_task >> update_stats
update_stats >> success_notification

# ä»»ä½•ä»»åŠ¡å¤±è´¥éƒ½è§¦å‘å‘Šè­¦
[extract_orders_task, clean_transform_task, join_dimensions_task,
 validate_quality_task, load_warehouse_task, update_stats] >> failure_alert
```

### ä»»åŠ¡ç»„ç®¡ç†

```python
# ä½¿ç”¨TaskGroupç»„ç»‡å¤æ‚ä»»åŠ¡
with TaskGroup('data_preparation', tooltip='æ•°æ®å‡†å¤‡é˜¶æ®µ') as prep_group:
    extract = PythonOperator(task_id='extract', ...)
    clean = PythonOperator(task_id='clean', ...)
    validate = PythonOperator(task_id='validate', ...)

    extract >> clean >> validate

with TaskGroup('data_loading', tooltip='æ•°æ®åŠ è½½é˜¶æ®µ') as load_group:
    load_staging = PythonOperator(task_id='load_staging', ...)
    load_prod = PythonOperator(task_id='load_prod', ...)
    update_metadata = PythonOperator(task_id='update_metadata', ...)

    load_staging >> load_prod >> update_metadata

prep_group >> load_group
```

## CDCå®æ—¶æ•°æ®æ•è·

### Debeziumæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Debezium CDC Architecture                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Source Database (MySQL)               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ orders   â”‚  users   â”‚ products â”‚   ...    â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚              binlog (Row Format)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚ Read binlog events               â”‚
â”‚                         v                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Debezium MySQL Connector                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚  â”‚  - Binlog Reader                         â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  - Event Parser                          â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  - Schema Registry Client                â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  - Offset Storage                        â”‚     â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚ Publish CDC events               â”‚
â”‚                         v                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Kafka Cluster                         â”‚    â”‚
â”‚  â”‚  Topic: dbserver1.inventory.orders                 â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”        â”‚    â”‚
â”‚  â”‚  â”‚ Part0 â”‚ Part1 â”‚ Part2 â”‚ Part3 â”‚ Part4 â”‚        â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Debezium Connectoré…ç½®

```json
{
  "name": "mysql-orders-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",

    "database.hostname": "mysql.prod.company.com",
    "database.port": "3306",
    "database.user": "debezium_user",
    "database.password": "${file:/secrets/db-password.txt:password}",
    "database.server.id": "184054",
    "database.server.name": "dbserver1",

    "database.include.list": "inventory",
    "table.include.list": "inventory.orders,inventory.order_items",

    "database.history.kafka.bootstrap.servers": "kafka1:9092,kafka2:9092,kafka3:9092",
    "database.history.kafka.topic": "dbhistory.inventory",

    "include.schema.changes": "true",
    "snapshot.mode": "when_needed",
    "snapshot.locking.mode": "minimal",

    "decimal.handling.mode": "precise",
    "time.precision.mode": "adaptive_time_microseconds",
    "bigint.unsigned.handling.mode": "precise",

    "event.processing.failure.handling.mode": "warn",
    "inconsistent.schema.handling.mode": "warn",

    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "true",

    "transforms": "unwrap,route",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": "false",
    "transforms.unwrap.delete.handling.mode": "rewrite",
    "transforms.unwrap.add.fields": "op,source.ts_ms,source.db,source.table",

    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "cdc.$3",

    "heartbeat.interval.ms": "10000",
    "heartbeat.topics.prefix": "__debezium-heartbeat",

    "skipped.operations": "t",

    "tombstones.on.delete": "true",

    "min.row.count.to.stream.results": "1000",
    "max.batch.size": "2048",
    "max.queue.size": "8192",
    "poll.interval.ms": "1000"
  }
}
```

### éƒ¨ç½²Debezium

```bash
# 1. å¯åŠ¨Kafka Connect
docker run -d \
  --name kafka-connect \
  --network=kafka-network \
  -p 8083:8083 \
  -e BOOTSTRAP_SERVERS=kafka1:9092,kafka2:9092,kafka3:9092 \
  -e GROUP_ID=debezium-cluster \
  -e CONFIG_STORAGE_TOPIC=debezium_connect_configs \
  -e OFFSET_STORAGE_TOPIC=debezium_connect_offsets \
  -e STATUS_STORAGE_TOPIC=debezium_connect_statuses \
  -e KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter \
  -e VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter \
  -e CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false \
  -e CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=true \
  debezium/connect:2.5

# 2. åˆ›å»ºConnector
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @mysql-orders-connector.json

# 3. æŸ¥çœ‹ConnectorçŠ¶æ€
curl http://localhost:8083/connectors/mysql-orders-connector/status | jq

# 4. æ¶ˆè´¹CDCäº‹ä»¶
kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic cdc.orders \
  --from-beginning \
  --property print.key=true
```

### CDCäº‹ä»¶æ ¼å¼

```json
{
  "before": null,
  "after": {
    "order_id": 1001,
    "user_id": 5001,
    "product_id": 3001,
    "quantity": 2,
    "price": 99.99,
    "total_amount": 199.98,
    "order_status": "paid",
    "payment_method": "credit_card",
    "created_at": "2026-02-07T10:30:00Z",
    "updated_at": "2026-02-07T10:30:00Z"
  },
  "source": {
    "version": "2.5.0.Final",
    "connector": "mysql",
    "name": "dbserver1",
    "ts_ms": 1707303000000,
    "snapshot": "false",
    "db": "inventory",
    "table": "orders",
    "server_id": 184054,
    "gtid": null,
    "file": "mysql-bin.000123",
    "pos": 456789,
    "row": 0,
    "thread": 7,
    "query": null
  },
  "op": "c",
  "ts_ms": 1707303000123,
  "transaction": null
}
```

## Flinkæµå¤„ç†

### Flink SQLå®æ—¶è®¢å•åˆ†æ

```sql
-- 1. åˆ›å»ºKafkaæºè¡¨ (è®¢å•CDCæµ)
CREATE TABLE orders_cdc (
    order_id BIGINT,
    user_id BIGINT,
    product_id BIGINT,
    quantity INT,
    price DECIMAL(10, 2),
    total_amount DECIMAL(10, 2),
    order_status STRING,
    payment_method STRING,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    op STRING,
    ts_ms BIGINT,
    proc_time AS PROCTIME(),
    event_time AS TO_TIMESTAMP(FROM_UNIXTIME(ts_ms / 1000)),
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'cdc.orders',
    'properties.bootstrap.servers' = 'kafka1:9092,kafka2:9092,kafka3:9092',
    'properties.group.id' = 'flink-orders-consumer',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'json',
    'json.fail-on-missing-field' = 'false',
    'json.ignore-parse-errors' = 'true'
);

-- 2. åˆ›å»ºç”¨æˆ·ç»´åº¦è¡¨
CREATE TABLE dim_users (
    user_id BIGINT,
    user_name STRING,
    user_level STRING,
    register_date DATE,
    city STRING,
    PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://mysql:3306/warehouse',
    'table-name' = 'dim_users',
    'username' = 'flink',
    'password' = 'password',
    'lookup.cache.max-rows' = '10000',
    'lookup.cache.ttl' = '1 hour'
);

-- 3. åˆ›å»ºäº§å“ç»´åº¦è¡¨
CREATE TABLE dim_products (
    product_id BIGINT,
    product_name STRING,
    category STRING,
    brand STRING,
    cost_price DECIMAL(10, 2),
    PRIMARY KEY (product_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://mysql:3306/warehouse',
    'table-name' = 'dim_products',
    'username' = 'flink',
    'password' = 'password',
    'lookup.cache.max-rows' = '50000',
    'lookup.cache.ttl' = '2 hour'
);

-- 4. å®æ—¶è®¢å•å®½è¡¨ (å…³è”ç»´åº¦)
CREATE VIEW enriched_orders AS
SELECT
    o.order_id,
    o.user_id,
    u.user_name,
    u.user_level,
    u.city,
    o.product_id,
    p.product_name,
    p.category,
    p.brand,
    o.quantity,
    o.price,
    p.cost_price,
    o.total_amount,
    (o.price - p.cost_price) * o.quantity AS profit,
    o.order_status,
    o.payment_method,
    o.created_at,
    o.event_time
FROM orders_cdc o
LEFT JOIN dim_users FOR SYSTEM_TIME AS OF o.proc_time AS u
    ON o.user_id = u.user_id
LEFT JOIN dim_products FOR SYSTEM_TIME AS OF o.proc_time AS p
    ON o.product_id = p.product_id
WHERE o.op IN ('c', 'r');  -- åªå¤„ç†INSERTå’ŒREADäº‹ä»¶

-- 5. å®æ—¶GMVç»Ÿè®¡ (æ»šåŠ¨çª—å£ - æ¯åˆ†é’Ÿ)
CREATE TABLE gmv_per_minute (
    window_start TIMESTAMP(3),
    window_end TIMESTAMP(3),
    order_count BIGINT,
    total_gmv DECIMAL(18, 2),
    avg_order_value DECIMAL(10, 2),
    PRIMARY KEY (window_start, window_end) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://mysql:3306/warehouse',
    'table-name' = 'rt_gmv_per_minute',
    'username' = 'flink',
    'password' = 'password',
    'sink.buffer-flush.max-rows' = '100',
    'sink.buffer-flush.interval' = '10s'
);

INSERT INTO gmv_per_minute
SELECT
    TUMBLE_START(event_time, INTERVAL '1' MINUTE) AS window_start,
    TUMBLE_END(event_time, INTERVAL '1' MINUTE) AS window_end,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_gmv,
    AVG(total_amount) AS avg_order_value
FROM enriched_orders
GROUP BY TUMBLE(event_time, INTERVAL '1' MINUTE);

-- 6. å®æ—¶å“ç±»é”€å”®æ’è¡Œ (æ»‘åŠ¨çª—å£ - 5åˆ†é’Ÿçª—å£, 1åˆ†é’Ÿæ»‘åŠ¨)
CREATE TABLE category_top_sales (
    window_start TIMESTAMP(3),
    window_end TIMESTAMP(3),
    category STRING,
    order_count BIGINT,
    total_sales DECIMAL(18, 2),
    total_profit DECIMAL(18, 2),
    ranking INT,
    PRIMARY KEY (window_start, window_end, category) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://mysql:3306/warehouse',
    'table-name' = 'rt_category_top_sales',
    'username' = 'flink',
    'password' = 'password'
);

INSERT INTO category_top_sales
SELECT
    window_start,
    window_end,
    category,
    order_count,
    total_sales,
    total_profit,
    ROW_NUMBER() OVER (
        PARTITION BY window_start, window_end
        ORDER BY total_sales DESC
    ) AS ranking
FROM (
    SELECT
        HOP_START(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE) AS window_start,
        HOP_END(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE) AS window_end,
        category,
        COUNT(*) AS order_count,
        SUM(total_amount) AS total_sales,
        SUM(profit) AS total_profit
    FROM enriched_orders
    WHERE category IS NOT NULL
    GROUP BY
        HOP(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE),
        category
)
WHERE ranking <= 10;

-- 7. å®æ—¶ç”¨æˆ·è¡Œä¸ºåˆ†æ (ä¼šè¯çª—å£)
CREATE TABLE user_session_analysis (
    user_id BIGINT,
    session_start TIMESTAMP(3),
    session_end TIMESTAMP(3),
    session_duration_minutes BIGINT,
    order_count BIGINT,
    total_spending DECIMAL(18, 2),
    PRIMARY KEY (user_id, session_start) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://mysql:3306/warehouse',
    'table-name' = 'rt_user_sessions',
    'username' = 'flink',
    'password' = 'password'
);

INSERT INTO user_session_analysis
SELECT
    user_id,
    SESSION_START(event_time, INTERVAL '30' MINUTE) AS session_start,
    SESSION_END(event_time, INTERVAL '30' MINUTE) AS session_end,
    TIMESTAMPDIFF(
        MINUTE,
        SESSION_START(event_time, INTERVAL '30' MINUTE),
        SESSION_END(event_time, INTERVAL '30' MINUTE)
    ) AS session_duration_minutes,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_spending
FROM enriched_orders
GROUP BY
    user_id,
    SESSION(event_time, INTERVAL '30' MINUTE);

-- 8. å®æ—¶å¼‚å¸¸æ£€æµ‹ (é‡‘é¢å¼‚å¸¸)
CREATE TABLE suspicious_orders (
    order_id BIGINT,
    user_id BIGINT,
    user_name STRING,
    product_id BIGINT,
    product_name STRING,
    quantity INT,
    price DECIMAL(10, 2),
    total_amount DECIMAL(18, 2),
    user_avg_spending DECIMAL(10, 2),
    deviation_ratio DECIMAL(10, 2),
    alert_time TIMESTAMP(3),
    alert_reason STRING,
    PRIMARY KEY (order_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://mysql:3306/warehouse',
    'table-name' = 'rt_suspicious_orders',
    'username' = 'flink',
    'password' = 'password'
);

-- è®¡ç®—ç”¨æˆ·å†å²å¹³å‡æ¶ˆè´¹
CREATE VIEW user_avg_spending AS
SELECT
    user_id,
    AVG(total_amount) OVER (
        PARTITION BY user_id
        ORDER BY event_time
        ROWS BETWEEN 100 PRECEDING AND 1 PRECEDING
    ) AS avg_spending
FROM enriched_orders;

-- æ£€æµ‹å¼‚å¸¸è®¢å•
INSERT INTO suspicious_orders
SELECT
    o.order_id,
    o.user_id,
    o.user_name,
    o.product_id,
    o.product_name,
    o.quantity,
    o.price,
    o.total_amount,
    u.avg_spending AS user_avg_spending,
    CASE
        WHEN u.avg_spending > 0
        THEN (o.total_amount - u.avg_spending) / u.avg_spending * 100
        ELSE 0
    END AS deviation_ratio,
    CURRENT_TIMESTAMP AS alert_time,
    CASE
        WHEN o.total_amount > u.avg_spending * 5 THEN 'è®¢å•é‡‘é¢å¼‚å¸¸é«˜'
        WHEN o.quantity > 50 THEN 'è´­ä¹°æ•°é‡å¼‚å¸¸'
        ELSE 'å…¶ä»–å¼‚å¸¸'
    END AS alert_reason
FROM enriched_orders o
JOIN user_avg_spending u ON o.user_id = u.user_id
WHERE
    o.total_amount > u.avg_spending * 5  -- é‡‘é¢è¶…è¿‡å¹³å‡å€¼5å€
    OR o.quantity > 50;  -- æ•°é‡è¶…è¿‡50
```

### Flink DataStream APIç¤ºä¾‹

```java
package com.company.flink;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

import java.time.Duration;

/**
 * å®æ—¶è®¢å•GMVç»Ÿè®¡
 */
public class OrderGMVAnalysis {

    public static void main(String[] args) throws Exception {
        // 1. åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(3);
        env.enableCheckpointing(60000); // 1åˆ†é’Ÿcheckpoint

        // 2. é…ç½®Kafka Source
        KafkaSource<String> source = KafkaSource.<String>builder()
            .setBootstrapServers("kafka1:9092,kafka2:9092,kafka3:9092")
            .setTopics("cdc.orders")
            .setGroupId("flink-gmv-consumer")
            .setStartingOffsets(OffsetsInitializer.latest())
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();

        // 3. åˆ›å»ºæ•°æ®æµ
        DataStream<String> orderStream = env
            .fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");

        // 4. è§£æJSONå¹¶è¿‡æ»¤
        DataStream<OrderEvent> parsedStream = orderStream
            .map(json -> parseOrderEvent(json))
            .filter(order -> order != null && order.getOp().equals("c"));

        // 5. è®¾ç½®Watermark
        DataStream<OrderEvent> watermarkedStream = parsedStream
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .<OrderEvent>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                    .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
            );

        // 6. æŒ‰åˆ†é’Ÿç»Ÿè®¡GMV
        DataStream<GMVResult> gmvStream = watermarkedStream
            .keyBy(order -> "global")
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .aggregate(new GMVAggregateFunction());

        // 7. è¾“å‡ºç»“æœ
        gmvStream.print();

        // 8. å†™å…¥MySQL
        gmvStream.addSink(new JdbcSink<>(/* JDBCé…ç½® */));

        // 9. æ‰§è¡Œä»»åŠ¡
        env.execute("Order GMV Analysis");
    }

    // GMVèšåˆå‡½æ•°
    public static class GMVAggregateFunction
            implements AggregateFunction<OrderEvent, GMVAccumulator, GMVResult> {

        @Override
        public GMVAccumulator createAccumulator() {
            return new GMVAccumulator();
        }

        @Override
        public GMVAccumulator add(OrderEvent order, GMVAccumulator acc) {
            acc.count++;
            acc.totalAmount += order.getTotalAmount();
            return acc;
        }

        @Override
        public GMVResult getResult(GMVAccumulator acc) {
            return new GMVResult(
                acc.count,
                acc.totalAmount,
                acc.totalAmount / acc.count
            );
        }

        @Override
        public GMVAccumulator merge(GMVAccumulator a, GMVAccumulator b) {
            a.count += b.count;
            a.totalAmount += b.totalAmount;
            return a;
        }
    }

    // ç´¯åŠ å™¨
    public static class GMVAccumulator {
        long count = 0;
        double totalAmount = 0.0;
    }
}
```

## æ•°æ®è´¨é‡æ£€æŸ¥

### Great Expectationsé…ç½®

```python
import great_expectations as ge
from great_expectations.core import ExpectationConfiguration
from great_expectations.data_context import DataContext

# åˆ›å»ºæ•°æ®ä¸Šä¸‹æ–‡
context = DataContext()

# åˆ›å»ºExpectation Suite
suite_name = "orders_quality_suite"
suite = context.create_expectation_suite(
    expectation_suite_name=suite_name,
    overwrite_existing=True
)

# å®šä¹‰æ•°æ®è´¨é‡è§„åˆ™
expectations = [
    # 1. ä¸»é”®å”¯ä¸€æ€§
    ExpectationConfiguration(
        expectation_type="expect_column_values_to_be_unique",
        kwargs={"column": "order_id"}
    ),

    # 2. éç©ºæ£€æŸ¥
    ExpectationConfiguration(
        expectation_type="expect_column_values_to_not_be_null",
        kwargs={"column": "user_id"}
    ),

    # 3. æ•°å€¼èŒƒå›´
    ExpectationConfiguration(
        expectation_type="expect_column_values_to_be_between",
        kwargs={
            "column": "quantity",
            "min_value": 1,
            "max_value": 1000
        }
    ),

    # 4. æšä¸¾å€¼æ£€æŸ¥
    ExpectationConfiguration(
        expectation_type="expect_column_values_to_be_in_set",
        kwargs={
            "column": "order_status",
            "value_set": ["pending", "paid", "shipped", "delivered", "cancelled"]
        }
    ),

    # 5. æ­£åˆ™è¡¨è¾¾å¼
    ExpectationConfiguration(
        expectation_type="expect_column_values_to_match_regex",
        kwargs={
            "column": "email",
            "regex": r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
        }
    ),

    # 6. åˆ—å­˜åœ¨æ€§
    ExpectationConfiguration(
        expectation_type="expect_table_columns_to_match_ordered_list",
        kwargs={
            "column_list": ["order_id", "user_id", "product_id", "quantity", "price"]
        }
    ),

    # 7. è¡Œæ•°æ£€æŸ¥
    ExpectationConfiguration(
        expectation_type="expect_table_row_count_to_be_between",
        kwargs={
            "min_value": 100,
            "max_value": 1000000
        }
    ),
]

# æ·»åŠ åˆ°Suite
for exp in expectations:
    suite.add_expectation(expectation_configuration=exp)

# ä¿å­˜Suite
context.save_expectation_suite(suite, suite_name)
```

## ç›‘æ§ä¸å‘Šè­¦

### Prometheusç›‘æ§æŒ‡æ ‡

```yaml
# airflow_exporteré…ç½®
---
apiVersion: v1
kind: Service
metadata:
  name: airflow-exporter
  labels:
    app: airflow
spec:
  ports:
  - port: 9112
    name: metrics
  selector:
    app: airflow
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: airflow-metrics
spec:
  selector:
    matchLabels:
      app: airflow
  endpoints:
  - port: metrics
    interval: 30s
```

### Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "æ•°æ®ç®¡é“ç›‘æ§",
    "panels": [
      {
        "title": "DAGæ‰§è¡ŒæˆåŠŸç‡",
        "targets": [{
          "expr": "rate(airflow_dag_run_success_total[5m]) / rate(airflow_dag_run_total[5m]) * 100"
        }]
      },
      {
        "title": "ä»»åŠ¡æ‰§è¡Œæ—¶é•¿",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(airflow_task_duration_seconds_bucket[5m]))"
        }]
      },
      {
        "title": "Kafkaæ¶ˆè´¹å»¶è¿Ÿ",
        "targets": [{
          "expr": "kafka_consumer_lag"
        }]
      },
      {
        "title": "Flink CheckpointæˆåŠŸç‡",
        "targets": [{
          "expr": "flink_jobmanager_job_lastCheckpointDuration / 1000"
        }]
      }
    ]
  }
}
```

## å®æˆ˜æ¡ˆä¾‹

### å®Œæ•´æ•°æ®ç®¡é“æ¡ˆä¾‹ï¼šç”¨æˆ·è¡Œä¸ºåˆ†æ

```
æ•°æ®æº --> CDC --> Kafka --> Flink --> æ•°æ®ä»“åº“ --> BIæŠ¥è¡¨

1. æ•°æ®é‡‡é›†
   - ç”¨æˆ·è¡Œä¸ºæ—¥å¿— (Nginx)
   - ä¸šåŠ¡æ•°æ®åº“ (MySQL CDC)
   - ç¬¬ä¸‰æ–¹APIæ•°æ®

2. å®æ—¶å¤„ç†
   - æ—¥å¿—è§£æä¸æ¸…æ´—
   - ä¼šè¯è¯†åˆ«ä¸å…³è”
   - å®æ—¶ç‰¹å¾è®¡ç®—

3. æ•°æ®å­˜å‚¨
   - åŸå§‹æ•°æ® --> HDFS
   - å¤„ç†æ•°æ® --> ClickHouse
   - ç»´åº¦æ•°æ® --> MySQL

4. æ•°æ®åº”ç”¨
   - å®æ—¶å¤§å±
   - æ¨èç³»ç»Ÿ
   - ç”¨æˆ·ç”»åƒ
```

## æ€»ç»“

æ•°æ®ç®¡é“è®¾è®¡çš„å…³é”®è¦ç´ ï¼š

1. **å¯é æ€§**ï¼šé‡è¯•æœºåˆ¶ã€æ•°æ®æ ¡éªŒã€ç›‘æ§å‘Šè­¦
2. **å¯æ‰©å±•æ€§**ï¼šåˆ†å¸ƒå¼æ¶æ„ã€å¹¶è¡Œå¤„ç†ã€å¼¹æ€§ä¼¸ç¼©
3. **å®æ—¶æ€§**ï¼šCDCæ•è·ã€æµå¼å¤„ç†ã€ç§’çº§å»¶è¿Ÿ
4. **æ•°æ®è´¨é‡**ï¼šè‡ªåŠ¨åŒ–æ£€æŸ¥ã€å¼‚å¸¸æ£€æµ‹ã€æ•°æ®æ²»ç†
5. **å¯ç»´æŠ¤æ€§**ï¼šæ¸…æ™°æ¶æ„ã€å®Œå–„æ–‡æ¡£ã€æ ‡å‡†åŒ–æµç¨‹

æ ¸å¿ƒå·¥å…·é“¾ï¼š
- **è°ƒåº¦ç¼–æ’**ï¼šAirflowã€DolphinScheduler
- **æ•°æ®é‡‡é›†**ï¼šDebeziumã€Maxwellã€Canal
- **æ¶ˆæ¯é˜Ÿåˆ—**ï¼šKafkaã€Pulsarã€RocketMQ
- **æµå¤„ç†**ï¼šFlinkã€Spark Streamingã€Kafka Streams
- **æ•°æ®è´¨é‡**ï¼šGreat Expectationsã€Deequã€Griffin
